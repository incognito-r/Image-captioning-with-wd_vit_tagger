{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model blip-large...\n",
      "Loading CLIP model ViT-H-14/openai...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pretrained weights (openai) not found for model ViT-H-14. Available pretrained tags (['laion2b_s32b_b79k', 'metaclip_fullcc', 'metaclip_altogether', 'dfn5b'].\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pretrained weights (openai) not found for model ViT-H-14. Available pretrained tags (['laion2b_s32b_b79k', 'metaclip_fullcc', 'metaclip_altogether', 'dfn5b'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ci = \u001b[43mInterrogator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mViT-H-14/openai\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\clip_interrogator\\clip_interrogator.py:71\u001b[39m, in \u001b[36mInterrogator.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m.clip_offloaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m.load_caption_model()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_clip_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\clip_interrogator\\clip_interrogator.py:105\u001b[39m, in \u001b[36mInterrogator.load_clip_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.quiet:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading CLIP model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.clip_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28mself\u001b[39m.clip_model, _, \u001b[38;5;28mself\u001b[39m.clip_preprocess = \u001b[43mopen_clip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_model_pretrained_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfp16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfp32\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_model_path\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.clip_model.eval()\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\open_clip\\factory.py:502\u001b[39m, in \u001b[36mcreate_model_and_transforms\u001b[39m\u001b[34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, load_weights_only, **model_kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_model_and_transforms\u001b[39m(\n\u001b[32m    473\u001b[39m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    474\u001b[39m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    492\u001b[39m         **model_kwargs,\n\u001b[32m    493\u001b[39m ):\n\u001b[32m    494\u001b[39m     force_preprocess_cfg = merge_preprocess_kwargs(\n\u001b[32m    495\u001b[39m         {},\n\u001b[32m    496\u001b[39m         mean=image_mean,\n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m         resize_mode=image_resize_mode,\n\u001b[32m    500\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     model = \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_weights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_weights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     pp_cfg = PreprocessCfg(**model.visual.preprocess_cfg)\n\u001b[32m    523\u001b[39m     preprocess_train = image_transform_v2(\n\u001b[32m    524\u001b[39m         pp_cfg,\n\u001b[32m    525\u001b[39m         is_train=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    526\u001b[39m         aug_cfg=aug_cfg,\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\open_clip\\factory.py:406\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, load_weights_only, **model_kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m         error_str = (\n\u001b[32m    403\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPretrained weights (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) not found for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    404\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Available pretrained tags (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_pretrained_tags_by_model(model_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    405\u001b[39m         logging.warning(error_str)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_str)\n\u001b[32m    407\u001b[39m     pretrained_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m has_hf_hub_prefix:\n",
      "\u001b[31mRuntimeError\u001b[39m: Pretrained weights (openai) not found for model ViT-H-14. Available pretrained tags (['laion2b_s32b_b79k', 'metaclip_fullcc', 'metaclip_altogether', 'dfn5b']."
     ]
    }
   ],
   "source": [
    "\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data/sample_images/000004.jpg\"\n",
    "image = Image.open(img_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 238.84it/s]\n",
      "Flavor chain:  38%|███▊      | 12/32 [00:24<00:41,  2.06s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 233.09it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 170.67it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 263.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a woman in a black leather jacket and a white bra, alessandra ambrosio, enormous hands, no watermark, in paris, wow, eyes closed or not visible, loosely cropped, flash photograph, creative commons attribution, drunk woman, claire forlani, by Walter Bayes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(ci.interrogate(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 144.18it/s]\n",
      "Flavor chain:  38%|███▊      | 12/32 [00:24<00:41,  2.06s/it]\n",
      "100%|██████████| 55/55 [00:00<00:00, 243.28it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 169.61it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 249.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a woman in a black leather jacket and a white bra, alessandra ambrosio, enormous hands, no watermark, in paris, wow, eyes closed or not visible, loosely cropped, flash photograph, creative commons attribution, drunk woman, claire forlani, by Walter Bayes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(ci.interrogate(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Salesforce/blip-image-captioning-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(img_path).convert('RGB')\n",
    "ci = Interrogator(Config(clip_model_name=\"openai/clip-vit-large-patch14\"))\n",
    "print(ci.interrogate(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['coca_base', 'coca_roberta-ViT-B-32', 'coca_ViT-B-32', 'coca_ViT-L-14', 'convnext_base', 'convnext_base_w', 'convnext_base_w_320', 'convnext_large', 'convnext_large_d', 'convnext_large_d_320', 'convnext_small', 'convnext_tiny', 'convnext_xlarge', 'convnext_xxlarge', 'convnext_xxlarge_320', 'EVA01-g-14', 'EVA01-g-14-plus', 'EVA02-B-16', 'EVA02-E-14', 'EVA02-E-14-plus', 'EVA02-L-14', 'EVA02-L-14-336', 'MobileCLIP-B', 'MobileCLIP-S1', 'MobileCLIP-S2', 'mt5-base-ViT-B-32', 'mt5-xl-ViT-H-14', 'nllb-clip-base', 'nllb-clip-base-siglip', 'nllb-clip-large', 'nllb-clip-large-siglip', 'RN50', 'RN50-quickgelu', 'RN50x4', 'RN50x4-quickgelu', 'RN50x16', 'RN50x16-quickgelu', 'RN50x64', 'RN50x64-quickgelu', 'RN101', 'RN101-quickgelu', 'roberta-ViT-B-32', 'swin_base_patch4_window7_224', 'ViT-B-16', 'ViT-B-16-plus', 'ViT-B-16-plus-240', 'ViT-B-16-quickgelu', 'ViT-B-16-SigLIP', 'ViT-B-16-SigLIP2', 'ViT-B-16-SigLIP2-256', 'ViT-B-16-SigLIP2-384', 'ViT-B-16-SigLIP2-512', 'ViT-B-16-SigLIP-256', 'ViT-B-16-SigLIP-384', 'ViT-B-16-SigLIP-512', 'ViT-B-16-SigLIP-i18n-256', 'ViT-B-32', 'ViT-B-32-256', 'ViT-B-32-plus-256', 'ViT-B-32-quickgelu', 'ViT-B-32-SigLIP2-256', 'ViT-bigG-14', 'ViT-bigG-14-CLIPA', 'ViT-bigG-14-CLIPA-336', 'ViT-bigG-14-quickgelu', 'ViT-e-14', 'ViT-g-14', 'ViT-gopt-16-SigLIP2-256', 'ViT-gopt-16-SigLIP2-384', 'ViT-H-14', 'ViT-H-14-378', 'ViT-H-14-378-quickgelu', 'ViT-H-14-CLIPA', 'ViT-H-14-CLIPA-336', 'ViT-H-14-quickgelu', 'ViT-H-16', 'ViT-L-14', 'ViT-L-14-280', 'ViT-L-14-336', 'ViT-L-14-336-quickgelu', 'ViT-L-14-CLIPA', 'ViT-L-14-CLIPA-336', 'ViT-L-14-quickgelu', 'ViT-L-16', 'ViT-L-16-320', 'ViT-L-16-SigLIP2-256', 'ViT-L-16-SigLIP2-384', 'ViT-L-16-SigLIP2-512', 'ViT-L-16-SigLIP-256', 'ViT-L-16-SigLIP-384', 'ViT-M-16', 'ViT-M-16-alt', 'ViT-M-32', 'ViT-M-32-alt', 'ViT-S-16', 'ViT-S-16-alt', 'ViT-S-32', 'ViT-S-32-alt', 'ViT-SO400M-14-SigLIP', 'ViT-SO400M-14-SigLIP2', 'ViT-SO400M-14-SigLIP2-378', 'ViT-SO400M-14-SigLIP-378', 'ViT-SO400M-14-SigLIP-384', 'ViT-SO400M-16-SigLIP2-256', 'ViT-SO400M-16-SigLIP2-384', 'ViT-SO400M-16-SigLIP2-512', 'ViT-SO400M-16-SigLIP-i18n-256', 'vit_medium_patch16_gap_256', 'vit_relpos_medium_patch16_cls_224', 'ViTamin-B', 'ViTamin-B-LTT', 'ViTamin-L', 'ViTamin-L2', 'ViTamin-L2-256', 'ViTamin-L2-336', 'ViTamin-L2-384', 'ViTamin-L-256', 'ViTamin-L-336', 'ViTamin-L-384', 'ViTamin-S', 'ViTamin-S-LTT', 'ViTamin-XL-256', 'ViTamin-XL-336', 'ViTamin-XL-384', 'xlm-roberta-base-ViT-B-32', 'xlm-roberta-large-ViT-H-14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
